{
  "metadata": {
    "name": "wcout3",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n# Leer archivos desde HDFS\r\nfiles_rdd \u003d sc.textFile(\"hdfs:///user/hadoop/datasets/gutenberg-small/*.txt\")\r\n\r\n# WordCount\r\nwc_unsort \u003d files_rdd.flatMap(lambda line: line.split()) \\\r\n                      .map(lambda word: (word, 1)) \\\r\n                      .reduceByKey(lambda a, b: a + b)\r\n\r\nwc \u003d wc_unsort.sortBy(lambda a: -a[1])\r\n\r\n# Mostrar las 10 palabras más frecuentes\r\nfor tupla in wc.take(10):\r\n    print(tupla)\r\n\r\n# Guardar el resultado en HDFS\r\nwc.coalesce(1).saveAsTextFile(\"hdfs:///tmp/wcout3\")\r\n"
    }
  ]
}